{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3352c55-74f5-4627-bffe-2a62646b9ca8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f56ae059e60c644d20fcf6566e83bde",
     "grade": false,
     "grade_id": "cell-e7f749f1d24233da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# IS319 - Deep Learning\n",
    "\n",
    "## TP1 - Neural networks\n",
    "\n",
    "The goal of this TP is to implement a simple feedforward neural network, but without the use of libraries like PyTorch or TensorFlow. We will only use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0a45a5ac-e124-4dd4-8c70-162cf1f303f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f27823f73c40d5370ea21b4664de6d15",
     "grade": false,
     "grade_id": "cell-00063860a6102415",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b494d22-a981-4cef-9a2f-180e2c03463c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fc7cdf13386929f1036ad16e37e9bb7",
     "grade": false,
     "grade_id": "cell-6677b191d92dc6b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1. Activation function and its derivative\n",
    "\n",
    "**(Question)** Implement the following activation function and its respective gradient (vector of partial derivatives). These should be applied element-wise to the input vector `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "977beeb6-6308-4a62-a6ec-cd5d7ef4f0af",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b450d1ed2934af7ada438471a86d694b",
     "grade": false,
     "grade_id": "cell-24345ebc30158580",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    '''Return the element-wise sigmoid of the input vector.'''\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "def d_sigmoid(a):\n",
    "    '''Return the partial derivatives of the sigmoid function\n",
    "    with respect to the input vector.'''\n",
    "    return sigmoid(a)*(1-sigmoid(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d135483f-3b82-44f3-b7be-469e71de5e42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "560266600065160cb781c5aca7376630",
     "grade": true,
     "grade_id": "activation-functions",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(100)\n",
    "assert np.all(sigmoid(a) >= 0.)\n",
    "assert np.all(sigmoid(a) <= 1.)\n",
    "assert sigmoid(0.) == 0.5\n",
    "assert np.all(d_sigmoid(a) >= 0.)\n",
    "assert np.all(d_sigmoid(a) <= 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2496f14-4f65-45aa-81de-7ed6776512f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "964d4939bd557cbe58eec84c4f663b6c",
     "grade": false,
     "grade_id": "cell-b5b0c0c62db57fb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2. Loss function and its derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e836c-55d0-47c7-a66a-9eadd6e808e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338407dbc743f6cc63b292b769db9dc2",
     "grade": false,
     "grade_id": "cell-cf207a1a25ede0bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Implement the following loss function and its respective gradient (vector of partial derivatives).\n",
    "\n",
    "`y` and `d` correspond to predictions and ground-truth labels respectively. They are assumed to be be matrices of size `n_classes * n_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f765ff51-e5f0-4639-bf94-b73ea1a4d729",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e354857200a6959d1c84a85536a5fe4",
     "grade": false,
     "grade_id": "cell-0570574f9ec3aec5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def squared_error(y, d):\n",
    "    '''Return a scalar corresponding to the sum of squared errors.'''\n",
    "    # The sum instead of mean will be more convenient for this TP\n",
    "    return np.sum((y - d)**2)\n",
    "def d_squared_error(y, d):\n",
    "    '''Return the vector of partial derivatives of the sum of\n",
    "    squared errors with respect to the predictions.'''\n",
    "    return 2*(y - d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "83c7a3fc-c76f-4d24-afca-25c409a8aff9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba56624405c27c0d63ad4f7591d6d1ae",
     "grade": true,
     "grade_id": "loss-function",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = np.random.randn(3, 100)\n",
    "d = np.random.randn(3, 100)\n",
    "assert squared_error(y, d) >= 0.\n",
    "assert d_squared_error(y, d).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfe238-cce1-447f-bc74-c92e2faff4b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "982d82edc67b00fc3f6a4478615ffca8",
     "grade": false,
     "grade_id": "cell-f5669aa56560f23f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 3. Neural network architecture\n",
    "\n",
    "We will implement a simple fully-connected neural network with **one hidden layer** and **one output layer**.\n",
    "\n",
    "This neural network is defined by a number of inputs, a number of hidden units, and a number of output units.\n",
    "\n",
    "The activation function will be sigmoid and the loss function will be the sum of squared errors, both implemented above.\n",
    "\n",
    "**(Question)** Complete the class below to initialize the weights and biases randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "34b35e8b-ca06-4e03-9121-5d012b89b40d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "682df5cc19b5bdfca32a46530e75b515",
     "grade": false,
     "grade_id": "cell-4ae365acb7330ae3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        '''Initialize a neural network with `n_input` input neurons,\n",
    "        `n_hidden` hidden neurons and `n_output` output neurons.'''\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        '''Initialize random weights with attributes `W1`, `b1`, `W2` and `b2`.'''\n",
    "        self.W1 = np.random.rand(self.n_hidden, self.n_input)\n",
    "        self.b1 = np.random.rand(self.n_hidden, 1)\n",
    "        self.W2 = np.random.rand(self.n_output, self.n_hidden)\n",
    "        self.b2 = np.random.rand(self.n_output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "98ee2249-0ffe-45b9-84aa-43df08dc2ffe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58074da67660f7a263cf4a5a217cdbc9",
     "grade": true,
     "grade_id": "init-weights",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "assert nn.W1.ndim == 2\n",
    "assert nn.b1.ndim == 2\n",
    "assert nn.W2.ndim == 2\n",
    "assert nn.b2.ndim == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd53bb-9867-436a-ba06-f12ab5c6ecb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e06363c60f3126a75881e357a54291b4",
     "grade": false,
     "grade_id": "cell-b38df17eaae875f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 4. Forward pass\n",
    "\n",
    "The forward pass is defined as:\n",
    "$$\\begin{align*}\n",
    "\\mathbf{h}_1 &= \\sigma(\\mathbf{a}_1) \\quad\\text{with}\\quad \\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1 \\\\\n",
    "\\mathbf{y} &= \\sigma(\\mathbf{a}_2) \\quad\\text{with}\\quad \\mathbf{a}_2 = \\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2\n",
    "\\end{align*}$$\n",
    "\n",
    "**(Question)** Implement the forward pass for input examples `X`. Save intermediate results `a1`, `h1` and `a2` into attributes (as they will be needed for the backpropagation algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0a17e7b8-f65d-46c5-8dbf-beaa9ae108e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5184d383ebc7f26abfd5b14931a7a9d9",
     "grade": false,
     "grade_id": "cell-ec6cc8adc2e96480",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork): # (the method will be added to the `NeuralNetwork` class)\n",
    "    def forward(self, X):\n",
    "        self.a1 = self.W1@X + self.b1\n",
    "        self.h1 = sigmoid(self.a1)\n",
    "        self.a2 = self.W2@self.h1 + self.b2\n",
    "        return sigmoid(self.a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a4ebfecd-ccf7-4d54-a817-7a8418733511",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fda0d7850901a8f80e7f34f12f1f0488",
     "grade": true,
     "grade_id": "forward-pass",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "y = nn.forward(X)\n",
    "assert y.shape == (3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776cbe4-c4dc-4022-bbb6-7a72ea9cd33f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "512aeaf857d711af6f945c86d86507db",
     "grade": false,
     "grade_id": "cell-a74aac18b5d769d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Implement the function below to obtain a classification decision from the network. To do that, apply the forward pass, then choose the class corresponding to the maximum output value.\n",
    "\n",
    "*Hint:* use the `np.argmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "51c63f4d-c588-4f53-a491-04b771a748ec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ceeb137463653213aa3a97d6cae7a073",
     "grade": false,
     "grade_id": "cell-07e51093c0c43eef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork): # (the method will be added to the `NeuralNetwork` class)\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "24649afc-05ba-458d-b221-65faf4708579",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1423d9148c0abdf24e6bb3d5f27f4b63",
     "grade": true,
     "grade_id": "predict",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "y = nn.predict(X)\n",
    "assert y.shape == (100,)\n",
    "assert np.any(y == 0) or np.any(y == 1) or np.any(y == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da06ec3-f507-4fff-8083-fee28c07f227",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10b0b1144dcc813fa37b813c6ae7ab05",
     "grade": false,
     "grade_id": "cell-716664c03ec25271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 5. Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810de3e4-4745-429d-b413-1fcd01ab78b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccb60940c6bf98203d784a85f79252e6",
     "grade": false,
     "grade_id": "cell-a379bc9c9644efe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Implement the backward pass for input examples `X`, ground-truth `d`, predictions `y`.\n",
    "\n",
    "*Advice:* keep track of the shapes of each derivative using comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2b0baa1a-0693-42d6-acb6-af801ddd3b38",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10bb98310ab5ee3faa32b668caa4d0d",
     "grade": false,
     "grade_id": "cell-dc848db6c6a52963",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def backward(self, X, y, d):\n",
    "        '''Compute the partial derivatives of the loss function\n",
    "        with respect to all weights of the neural network.\n",
    "        Return these in variables `d_W1`, `d_b1`, `d_W2` and `d_b2`.'''\n",
    "        # Backpropagation for the output layer\n",
    "        # You should compute, d_ey, d_ya2, d_a2w2 and finally delta2\n",
    "        # Then you can compute d_W2 and d_b2\n",
    "        d_ey = d_squared_error(y, d)\n",
    "        d_ya2 = d_sigmoid(self.a2)\n",
    "        d_a2w2 = self.h1\n",
    "        delta2 = d_ey*d_ya2\n",
    "        d_W2 = delta2@d_a2w2.T\n",
    "        d_b2 = np.sum(delta2, axis=1, keepdims=True)\n",
    "\n",
    "        \n",
    "        # Backpropagation for the hidden layer\n",
    "        # You should comput d_h1a1 and finally delta1\n",
    "        # Then you can compute d_W1 and d_b1\n",
    "        d_h1a1 = d_sigmoid(self.a1)\n",
    "        d_a1w1 = X\n",
    "        delta1 = np.multiply(self.W2.T@delta2, d_h1a1)\n",
    "        d_W1 = delta1@d_a1w1.T\n",
    "        d_b1 = np.sum(delta1, axis=1, keepdims=True)\n",
    "\n",
    "        \n",
    "        return d_W1, d_b1, d_W2, d_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d48b45f8-e1ad-4995-b0f7-ebf2f4a5af1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43dacaabd4b1f99cecd67d48bc087b68",
     "grade": true,
     "grade_id": "backward",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "d = np.random.randint(0, 2, size=(3, 100))\n",
    "y = nn.forward(X)\n",
    "loss = squared_error(y, d)\n",
    "d_W1, d_b1, d_W2, d_b2 = nn.backward(X, y, d)\n",
    "assert d_W1.shape == nn.W1.shape\n",
    "assert d_b1.shape == nn.b1.shape\n",
    "assert d_W2.shape == nn.W2.shape\n",
    "assert d_b2.shape == nn.b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24a0eb-ef0d-4de5-991e-07d6bf058af5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0923b4500a605d7dfc5b4d58460af318",
     "grade": false,
     "grade_id": "cell-0f7f2ccc7c2703a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 6. Weights update with gradient descent\n",
    "\n",
    "**(Question)** Complete the following code to implement one iteration of the training process:\n",
    "- Apply the forward pass on training data and compute the loss\n",
    "- Apply backpropagation to compute the gradient of the loss with respect to the network parameters\n",
    "- Apply gradient descent to update the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "afbfeefb-6232-4b1a-9662-2db27ee41321",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd517e2969c8ef56d55535f60e75448e",
     "grade": false,
     "grade_id": "cell-064008dd04d210c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def train_iteration(self, X, d, lr=1e-2):\n",
    "        # Apply forward pass and compute the loss\n",
    "        y = self.forward(X)\n",
    "        loss = squared_error(y, d)\n",
    "        # Apply backpropagation to compute the gradients\n",
    "        d_W1, d_b1, d_W2, d_b2 = self.backward(X, y, d)\n",
    "           \n",
    "        # Apply gradient descent to update the weights\n",
    "        self.W1 = self.W1 - lr*d_W1\n",
    "        self.b1 = self.b1 - lr*d_b1\n",
    "        self.W2 = self.W2 - lr*d_W2\n",
    "        self.b2 = self.b2 - lr*d_b2 \n",
    "        loss = squared_error(y, d)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9e69d878-5a2e-457d-b0ad-76f2915909a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c986a85fe63184deffcf06225b1a750",
     "grade": true,
     "grade_id": "train-iteration",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "d = np.random.randint(0, 2, size=(3, 100))\n",
    "loss = nn.train_iteration(X, d, lr=100)\n",
    "assert loss >= 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375846fe-7894-4467-9245-ce02b845d587",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2be8e2aac2709ddb02ba084c50bf7ff4",
     "grade": false,
     "grade_id": "cell-f6f5fa4a0e6feedc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 7. Mini-batch training loop\n",
    "\n",
    "Now, we will implement the main training loop of our neural network.\n",
    "\n",
    "We will use stochastic gradient descent with mini-batch: the weights will be updated by performing gradient descent on shuffled subsets of training data.\n",
    "\n",
    "We will train the network for a number of epochs (an epoch is performed when the whole training set has been used with this mini-batch procedure).\n",
    "\n",
    "**(Question)** Complete the code below to implement the training loop with minibatch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "130674e9-0101-4192-baf0-312abdbd0134",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93c2d09dd8e052523c7de23ad4901c12",
     "grade": false,
     "grade_id": "cell-d38adca6962df05a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def fit(self, X, d, batch_size, n_epochs=10, lr=1e-2):\n",
    "        n_samples = X.shape[1]\n",
    "        n_batches = (n_samples // batch_size) + 1\n",
    "        epoch_loss = 0.\n",
    "        for e in range(n_epochs):\n",
    "            # Shuffle dataset\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X, d = X[:, permutation], d[:, permutation]\n",
    "            # Loop over each batch\n",
    "            for b in range(0, n_samples, batch_size): # range(start, stop, step)\n",
    "                # Grab the current batch in `X_batch` and `d_batch`\n",
    "                X_batch = X[:, b:b+batch_size]\n",
    "                d_batch = d[:, b:b+batch_size]\n",
    "                # Apply training iteration and update epoch loss\n",
    "                epoch_loss += self.train_iteration(X_batch, d_batch, lr)\n",
    "        # Compute average epoch loss and print it\n",
    "        print(f\"Average epoch loss: {epoch_loss/(n_epochs*n_batches)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116cae8-1f48-45a4-b1d7-bf5cd35e97d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "310cefcc668c25ded2cfc87b04cf2434",
     "grade": false,
     "grade_id": "cell-c980fa7b2fa057b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 8. Train the network on the MNIST dataset\n",
    "\n",
    "The MNIST dataset is composed of 70000 greyscale images of handwritten digits: 60000 images for training and 10000 for testing.\n",
    "\n",
    "It is included in the `mnist.tgz` archive provided with this TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8e1e3e19-c599-4633-b815-a5af3afe1cdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef0c69a8ba2ce881cb101b8a63d2ef5f",
     "grade": false,
     "grade_id": "cell-5562efce16521bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist-test-images.npy\n",
      "mnist-test-labels.npy\n",
      "mnist-train-images.npy\n",
      "mnist-train-labels.npy\n"
     ]
    }
   ],
   "source": [
    "!tar xvzf ./mnist.tgz\n",
    "images_train = np.load('./mnist-train-images.npy')\n",
    "labels_train = np.load('./mnist-train-labels.npy')\n",
    "images_test = np.load('./mnist-test-images.npy')\n",
    "labels_test = np.load('./mnist-test-labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60674f9f-1b2d-481c-8d9f-6427ea608d93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4acf046d404e63e8f9cf998a58328f22",
     "grade": false,
     "grade_id": "cell-c3a9fec3fa080314",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Reshape the images into vectors and normalize the pixel values between 0 and 1. Convert the labels into one-hot vectors (*i.e.* vectors full of 0 and with only a 1 for the corresponding class). Store the results into `X_train`, `y_train`, `X_test` and `y_test` variables. Make sure to reshape to the following:\n",
    "- Input data: `n_features x n_samples`\n",
    "- Labels: `n_classes x n_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "84e60aea-3db1-4bb2-a394-ce7f51ac3ca8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71b3aa5261b79b669d6a76c5634840a7",
     "grade": false,
     "grade_id": "cell-458517fdffea414b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples_train = images_train.shape[0]\n",
    "n_samples_test = images_test.shape[0]\n",
    "X_train = np.moveaxis(images_train.reshape(n_samples_train, -1), 0, -1)\n",
    "X_test = np.moveaxis(images_test.reshape(n_samples_test, -1), 0, -1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "n_classes = 10\n",
    "\n",
    "#one hot encoding labels\n",
    "y_train = np.zeros((n_classes, n_samples_train))\n",
    "y_test = np.zeros((n_classes, n_samples_test))\n",
    "for i in range(n_samples_train):\n",
    "    y_train[labels_train[i], i] = 1\n",
    "for i in range(n_samples_test):\n",
    "    y_test[labels_test[i], i] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b753e345-4d69-47d3-bc2d-4a05f4c09766",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "978093784e5ba013a3a0636d87082cf1",
     "grade": true,
     "grade_id": "mnist-dataset",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.all(X_train >= 0.) and np.all(X_train <= 1.)\n",
    "assert np.all(X_test >= 0.) and np.all(X_test <= 1.)\n",
    "assert np.all(np.unique(y_train) == np.array([0., 1.])) \n",
    "assert np.all(np.unique(y_test) == np.array([0., 1.]))\n",
    "assert np.all(np.sum(y_train, axis=0) == 1.)\n",
    "assert np.all(np.sum(y_test, axis=0) == 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c092ea-0342-4d64-89b9-c8ed5a1a2ad4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd5c0f6c9bd960d9ea0b3cc726d41e90",
     "grade": false,
     "grade_id": "cell-6c0d93b6eb6561df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Initialize a neural network for MNIST with 32 hidden units and train it for 10 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77f0664e-95cb-4b4f-a711-ca5d7b067df2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c48d1d1654c955db7f9263ad1c5abc6f",
     "grade": false,
     "grade_id": "cell-3cc325c6ca9e4632",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 4576.269698322493\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(28*28, 32, 10)\n",
    "nn.fit(X_train, y_train, 512, 10, 10e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f971d1f-a2be-4e14-8b32-d16cf95133bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0702549a613d49feff7dfd5b4178c833",
     "grade": false,
     "grade_id": "cell-c428e777c7ce4fc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Compute the classification accuracy on the train and test sets. To do that, you can use the predict function and compare them with the original labels (*i.e.* without one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c0079310-dc37-4e09-a3d8-802d20f5c9ca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9618f35d173603514e09391c99928a38",
     "grade": false,
     "grade_id": "cell-7b50de5211771a12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_training = nn.predict(X_train)\n",
    "pred_test = nn.predict(X_test)\n",
    "accuracy_train = np.sum(pred_training == labels_train) / n_samples_train\n",
    "accuracy_test = np.sum(pred_test == labels_test) / n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6f6902cd-b387-4a8a-a984-fd814651bcf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df410b8867bd826543e85965cad6cdab",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(0. <= accuracy_train <= 1.)\n",
    "assert(0. <= accuracy_test <= 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0b13d-ff16-4a77-b031-d6b770d8d542",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1460ad42591ceee6c3ad8d6cd56e8f2",
     "grade": false,
     "grade_id": "cell-d57ca20154998a71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Compute and plot the confusion matrix for the test set. Which are the most difficult classes? Show some examples of misclassified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd429c3b-0caa-4404-abd1-90634a141887",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51b132b4eceadd782d22bae82c25bc98",
     "grade": false,
     "grade_id": "cell-a412a782fabb39c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe524c00100>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Confusion matrix for test set\n",
    "cm = confusion_matrix(labels_test, pred_test)\n",
    "plt.matshow(cm, cmap='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2034c-1258-4e01-945f-6f1b5907dd7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68541cac2f30c21ce5fa54188105962c",
     "grade": false,
     "grade_id": "cell-7d1885888535c8ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Play around with hyperparameters of the model. What happens when the batch size if very small? And very large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6dd58d50-39f8-401b-b84e-b3cf5c060cf6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a262e4a7afc1871ac16225f72c9bc7b7",
     "grade": true,
     "grade_id": "cell-b6db4b1e04482447",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 4576.268598503578\n",
      "Accuracy on training set: 0.09751666666666667\n",
      "Accuracy on test set: 0.0974\n",
      "Average epoch loss: 4576.270067033373\n",
      "Accuracy on training set: 0.11236666666666667\n",
      "Accuracy on test set: 0.1135\n",
      "Average epoch loss: 287.8463986692248\n",
      "Accuracy on training set: 0.09863333333333334\n",
      "Accuracy on test set: 0.0958\n"
     ]
    }
   ],
   "source": [
    "# Different hyperparameters\n",
    "nn = NeuralNetwork(28*28, 32, 10)\n",
    "nn.fit(X_train, y_train, 512, 10, 10e-5)\n",
    "pred_training = nn.predict(X_train)\n",
    "pred_test = nn.predict(X_test)\n",
    "accuracy_train = np.sum(pred_training == labels_train) / n_samples_train\n",
    "accuracy_test = np.sum(pred_test == labels_test) / n_samples_test\n",
    "print(f\"Accuracy on training set: {accuracy_train}\")\n",
    "print(f\"Accuracy on test set: {accuracy_test}\")\n",
    "# Different learning rate\n",
    "nn1 = NeuralNetwork(28*28, 32, 10)\n",
    "nn1.fit(X_train, y_train, 512, 10, 10e-4)\n",
    "pred_training = nn1.predict(X_train)\n",
    "pred_test = nn1.predict(X_test)\n",
    "accuracy_train = np.sum(pred_training == labels_train) / n_samples_train\n",
    "accuracy_test = np.sum(pred_test == labels_test) / n_samples_test\n",
    "print(f\"Accuracy on training set: {accuracy_train}\")\n",
    "print(f\"Accuracy on test set: {accuracy_test}\")\n",
    "#Small batch size\n",
    "nn2 = NeuralNetwork(28*28, 32, 10)\n",
    "nn2.fit(X_train, y_train, 32, 10, 10e-5)\n",
    "pred_training = nn2.predict(X_train)\n",
    "pred_test = nn2.predict(X_test)\n",
    "accuracy_train = np.sum(pred_training == labels_train) / n_samples_train\n",
    "accuracy_test = np.sum(pred_test == labels_test) / n_samples_test\n",
    "print(f\"Accuracy on training set: {accuracy_train}\")\n",
    "print(f\"Accuracy on test set: {accuracy_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5c9c3-b3fc-424c-b815-83c47a704229",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "143b011e40fc3c36f3d281586f34ba00",
     "grade": true,
     "grade_id": "cell-dd69c2dd2d0a35d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "For small batch size we observe very small average epoch loss compared to other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccc60a-4f7d-4e31-827f-39b2791c1713",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "585921652da5e2beb11f7aca9c9721b0",
     "grade": false,
     "grade_id": "cell-60e9495352b554be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 9. (BONUS) Extension to softmax and categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d36216-95f5-4db1-b83d-39942e09df34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e7cadb0c018a18655fd2664c7057b5",
     "grade": false,
     "grade_id": "cell-de5dfffbcb5a2be4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "**(Question)** Extend your neural network model to use a softmax activation function for the output layer, and a categorical cross-entropy loss.\n",
    "You can also experiment with the reLU activation for the hidden layer.\n",
    "\n",
    "*Hint:* recall the partial derivatives formulation from logistic regression, and optimize the backpropagation for the output layer accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "96c24019-4f36-497f-ad3e-a42ce1a8ad79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "def cross_entropy_loss(y, d):\n",
    "    return -np.sum(d * np.log(y))\n",
    "#derivative of cross entropy loss\n",
    "def d_cross_entropy_loss(y, d):\n",
    "    return y - d\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        '''Initialize a neural network with `n_input` input neurons,\n",
    "        `n_hidden` hidden neurons and `n_output` output neurons.'''\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        '''Initialize random weights with attributes `W1`, `b1`, `W2` and `b2`.'''\n",
    "        self.W1 = np.random.rand(self.n_hidden, self.n_input)\n",
    "        self.b1 = np.random.rand(self.n_hidden, 1)\n",
    "        self.W2 = np.random.rand(self.n_output, self.n_hidden)\n",
    "        self.b2 = np.random.rand(self.n_output, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a1 = self.W1@X + self.b1\n",
    "        self.h1 = ReLU(self.a1)\n",
    "        self.a2 = self.W2@self.h1 + self.b2\n",
    "        return softmax(self.a2)\n",
    "\n",
    "    def backward(self, X, y, d):\n",
    "        '''Compute the partial derivatives of the loss function\n",
    "        with respect to all weights of the neural network.\n",
    "        Return these in variables `d_W1`, `d_b1`, `d_W2` and `d_b2`.'''\n",
    "        # Backpropagation for the output layer\n",
    "        # You should compute, d_ey, d_ya2, d_a2w2 and finally delta2\n",
    "        # Then you can compute d_W2 and d_b2\n",
    "        d_ey = d_squared_error(y, d)\n",
    "        d_ya2 = d_sigmoid(self.a2)\n",
    "        d_a2w2 = self.h1\n",
    "        delta2 = d_ey*d_ya2\n",
    "        d_W2 = delta2@d_a2w2.T\n",
    "        d_b2 = np.sum(delta2, axis=1, keepdims=True)\n",
    "\n",
    "        \n",
    "        # Backpropagation for the hidden layer\n",
    "        # You should comput d_h1a1 and finally delta1\n",
    "        # Then you can compute d_W1 and d_b1\n",
    "        d_h1a1 = d_sigmoid(self.a1)\n",
    "        d_a1w1 = X\n",
    "        delta1 = np.multiply(self.W2.T@delta2, d_h1a1)\n",
    "        d_W1 = delta1@d_a1w1.T\n",
    "        d_b1 = np.sum(delta1, axis=1, keepdims=True)\n",
    "\n",
    "    def train_iteration(self, X, d, lr=1e-2):\n",
    "        # Apply forward pass and compute the loss\n",
    "        y = self.forward(X)\n",
    "        loss = squared_error(y, d)\n",
    "        # Apply backpropagation to compute the gradients\n",
    "        d_W1, d_b1, d_W2, d_b2 = self.backward(X, y, d)\n",
    "           \n",
    "        # Apply gradient descent to update the weights\n",
    "        self.W1 = self.W1 - lr*d_W1\n",
    "        self.b1 = self.b1 - lr*d_b1\n",
    "        self.W2 = self.W2 - lr*d_W2\n",
    "        self.b2 = self.b2 - lr*d_b2 \n",
    "        loss = squared_error(y, d)\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, d, batch_size, n_epochs=10, lr=1e-2):\n",
    "        n_samples = X.shape[1]\n",
    "        n_batches = (n_samples // batch_size) + 1\n",
    "        epoch_loss = 0.\n",
    "        for e in range(n_epochs):\n",
    "            # Shuffle dataset\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X, d = X[:, permutation], d[:, permutation]\n",
    "            # Loop over each batch\n",
    "            for b in range(0, n_samples, batch_size): # range(start, stop, step)\n",
    "                # Grab the current batch in `X_batch` and `d_batch`\n",
    "                X_batch = X[:, b:b+batch_size]\n",
    "                d_batch = d[:, b:b+batch_size]\n",
    "                # Apply training iteration and update epoch loss\n",
    "                epoch_loss += self.train_iteration(X_batch, d_batch, lr)\n",
    "        # Compute average epoch loss and print it\n",
    "        print(f\"Average epoch loss: {epoch_loss/(n_epochs*n_batches)}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9f104-9ab8-46f1-97a3-93b714e6b018",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da3e081a9f0624df3655ec5890bebb82",
     "grade": false,
     "grade_id": "cell-4dccedea9995943c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "**(Question)** Extend your neural network model to handle more than one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557a958-dbdb-44cd-9063-d22b68d3d9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
